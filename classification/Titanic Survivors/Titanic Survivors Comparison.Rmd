---
title: "Titanic Survivor Case Study"
subtitle: "A Comparison of Classification Algorithms"
author: "Mike Silva"
date: "April 4th, 2015"
output: html_document
---

# Introduction

In this study we compare classification models built in R using different algorithms that predict the likelihood of emerging as a Titanic survivor.  It compares the models in terms of accuracy and execution time.  Given that a trade-off exists between the two, we present our comparison weighing each factor differently.  That way an individual can evaluate the model with their own willingness to spend some extra time in order to get a more accurate model.

## Data Used in This Study

The survivor data used in this analyis is found in [Carnegie Mellon University Department of Statitics's StatLib](http://lib.stat.cmu.edu/S/Harrell/data/descriptions/titanic.html).  This dataset describes the survival status of individual passengers on the Titanic.  It should be noted that the crew are excluded from this dataset.

The principal source for data about Titanic passengers is the *Encyclopedia Titanica* available online at http://atschool.eduweb.co.uk/phind. The datasets used here were begun by a variety of researchers. One of the original sources is Eaton & Haas (1994) *Titanic: Triumph and Tragedy*, Patrick Stephens Ltd, which includes a passenger list created by many researchers and edited by Michael A. Findlay.

Thomas Cason of UVa has greatly updated and improved the original titanic passenger list.  This list does not have the crew but it does contain
actual and estimated ages for almost 80% of the passengers.  To learn more about how the data was assempled, I would recommend you read http://lib.stat.cmu.edu/S/Harrell/data/descriptions/titanic3info.txt.

In order to pull in the data the follwing commands were executed:

```{r, message=FALSE}
# Download Excel file if not present
if(!file.exists('titanic3.xls')){
  download.file('http://lib.stat.cmu.edu/S/Harrell/data/xls/titanic3.xls', 'titanic3.xls')
}

# Read the data into a data frame
library(plyr) # This will be needed latter but needs to be loaded before dplyr
library(dplyr)
library(XLConnect)

titanic <- loadWorkbook('titanic3.xls') %>%
  readWorksheet(., sheet='titanic3')
```

There are `r nrow(titanic)` records in the dataset and `r ncol(titanic)` variables. The variables in the extracted dataset are `r names(titanic)`.  pclass refers to passenger class (1st, 2nd, 3rd), and is a proxy for socio-economic class. Age is in years and some infants had fractional values.  For clarity we will convert the survived variable from a boolean to a categorical.

```{r, message=FALSE}
titanic <- titanic %>%
  mutate(survived = as.factor(ifelse(survived==1,'Yes','No')))
```

If you are unfamiliar with this dataset you can view the whole thing in the Appendix section of this paper.

# Data Processing

We want the models to based on characteristics of the passenger.  We will be removing the name of the individual, ticket number, cabin and home/destination variables as they do not have any predictive value.  The lifeboat and body identification number will also be dropped because they don't tell us much about the person.  We will also only select the records that have complete information.  Some of the variables are closely correlated (i.e. pclass and fare) but I have chosen to leave them in.

```{r}
study.variables <- c('pclass', 'survived', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked')
# Select the study variables and drop all other variables
study.data <- titanic[, study.variables]
# Remove the observations with NA's
study.data <- study.data[complete.cases(study.data),]
```

The study dataset contains `r nrow(study.data)` and the `r ncol(study.data)` variables defined above.  The study data was divided into training and test sets using an 80/20 split. In order to make these results reproducible, a random number seed is defined which will be used throughout this study.

```{r, message=FALSE}
library(caret)
# Set the random number seed
study.seed <- 12345
set.seed(study.seed)
in.train <- createDataPartition(y=study.data$survived, times=1, p=0.80, list=FALSE)
training <- study.data[in.train,]
test <- study.data[-in.train,]
```

# Baseline Model - Women and Children First

In order to evaluate the accuracy of a model I needed a baseline.  We know the people who survived the sinking of the Titanic were those who were able to get a spot on a life boat.  One general rule on who get's a spot is "women and children first."  I will use stictly apply this this rule as the baseline defining a child as a person under 18 years of age:

```{r}
# Women and Children First rule
survived.rule <- function(age, sex){
  if(sex == 'female' | age < 18){
    return('Yes')
  }
  else{
    return('No')
  }
}
```

I will then apply the baseline model to form predictions:

```{r}
start <- proc.time()
baseline.predictions <- test %>%
  select(age, sex) %>%
  apply(., 1, function(x){survived.rule(x[1],x[2])})
baseline.time <- proc.time() - start
```

I will then evaluate the accuracy of this baseline model by comparing the number of times it correctly predicted the outcome relative to the total number of cases:  

```{r, message=FALSE}
baseline.confusion.matrix <- test %>%
  select(survived) %>%
  cbind(., baseline.predictions) %>%
  mutate(truth = as.character(survived)) %>%
  mutate(model = as.character(baseline.predictions)) %>%
  mutate(count=1) %>%
  xtabs(count~model+truth, data=.) %>%
  confusionMatrix(.)

baseline.confusion.matrix
```

## Reliability of the Baseline Model

The "women and children first" model has an accuracy of `r round(baseline.confusion.matrix$overall[1]*100,2)`% and a kappa of `r round(baseline.confusion.matrix$overall[2]*100,2)`%.

# Machine Learning Models
Now that a baseline has been established we will compare it to other machine learning models and observe how it preforms against the baseline.  

I will be using R's caret package to develop eight models.  Each model will have 3 repeats of a 10-fold cross validation or 30 results.  We will use a common random number seed to ensure that the models get the same data partitions and repeats.  We will compare the average accuracy and kappa of these models with the baseline discussed in the previous section.  The algorythms were selected in no specific order.

```{r, message=FALSE}
# Set up the 3 repeats and 10-fold cross validation
study.folds <- 10
study.repeats <- 3
study.control <- trainControl(method='repeatedcv', number=study.folds, repeats=study.repeats)
```

## Model 1: Random Forest
```{r, message=FALSE}
set.seed(study.seed)
start <- proc.time()
rf.model <- train(survived~., training, method='rf', trControl=study.control)
rf.prediction <- predict(rf.model, newdata=test)
rf.confusion.matrix <- confusionMatrix(rf.prediction, test$survived)
rf.time <- proc.time() - start
rf.confusion.matrix
```

## Model 2: Learning Vector Quantization
```{r, message=FALSE}
set.seed(study.seed)
start <- proc.time()
lvq.model <- train(survived~., study.data, method='lvq', trControl=study.control)
lvq.prediction <- predict(lvq.model, newdata=test)
lvq.confusion.matrix <- confusionMatrix(lvq.prediction, test$survived)
lvq.time <- proc.time() - start
lvq.confusion.matrix
```

## Model 3: Support Vector Machine
```{r, message=FALSE}
set.seed(study.seed)
start <- proc.time()
svm.model <- train(survived~., study.data, method='svmRadial', trControl=study.control)
svm.prediction <- predict(svm.model, newdata=test)
svm.confusion.matrix <- confusionMatrix(svm.prediction, test$survived)
svm.time <- proc.time() - start
svm.confusion.matrix
```

## Model 4: Gradient Boosted Machine
```{r, message=FALSE}
set.seed(study.seed)
start <- proc.time()
gbm.model <- train(survived~., study.data, method='gbm', trControl=study.control, verbose=FALSE)
gbm.prediction <- predict(gbm.model, newdata=test)
gbm.confusion.matrix <- confusionMatrix(gbm.prediction, test$survived)
gbm.time <- proc.time() - start
gbm.confusion.matrix
```

## Model 5: Naive Bayes
```{r, message=FALSE, warning=FALSE}
set.seed(study.seed)
start <- proc.time()
nb.model <- train(survived~., study.data, method='nb', trControl=study.control)
nb.prediction <- predict(nb.model, newdata=test)
nb.confusion.matrix <- confusionMatrix(nb.prediction, test$survived)
nb.time <- proc.time() - start
nb.confusion.matrix
```

## Model 6: Neural Network
```{r, message=FALSE, results='hide'}
set.seed(study.seed)
start <- proc.time()
nnet.model <- train(survived~., study.data, method='nnet', trControl=study.control)
nnet.prediction <- predict(nnet.model, newdata=test)
nnet.confusion.matrix <- confusionMatrix(nnet.prediction, test$survived)
nnet.time <- proc.time() - start
nnet.confusion.matrix
```

## Model 7: Classification and Regression Trees
```{r, message=FALSE}
set.seed(study.seed)
start <- proc.time()
rpart.model <- train(survived~., study.data, method='rpart', trControl=study.control)
rpart.prediction <- predict(rpart.model, newdata=test)
rpart.confusion.matrix <- confusionMatrix(rpart.prediction, test$survived)
rpart.time <- proc.time() - start
rpart.confusion.matrix
```

## Model 8: K-Nearest Neighbors
```{r, message=FALSE}
set.seed(study.seed)
start <- proc.time()
knn.model <- train(survived~., study.data, method='knn', trControl=study.control)
knn.prediction <- predict(knn.model, newdata=test)
knn.confusion.matrix <- confusionMatrix(knn.prediction, test$survived)
knn.time <- proc.time() - start
knn.confusion.matrix
```

# Comparison of Machine Learning Models

Now that the models have been build we can examing the results.  First we need to compare the accuracy of the model.  We will examine both the accuracy and Cohen's kappa.

First I will assemble the runtime information into a data frame.  I will need to scale up the baseline time because the runtimes for the other models represent the time needed to make 30 models.  I will to this by multiplying the baseline runtime by thirty.  There was also time taken to program the model.  I will be generous and say it took a minute to program the baseline survivor rule function:

```{r}
baseline.program.time <- 60
baseline.scaler <- study.folds * study.repeats

model.name <- c('Baseline', 'Gradient Boosted Machine', 'K Nearest Neighbors', 'Learning Vector Quantization', 'Naive Bayes', 'Neural Networks', 'Random Forest', 'Classification and Regression Tree','Support Vector Machine')

accuracy <- c(baseline.confusion.matrix$overall[1], gbm.confusion.matrix$overall[1], knn.confusion.matrix$overall[1],lvq.confusion.matrix$overall[1], nb.confusion.matrix$overall[1], nnet.confusion.matrix$overall[1], rf.confusion.matrix$overall[1], rpart.confusion.matrix$overall[1], svm.confusion.matrix$overall[1])

kappa <- c(baseline.confusion.matrix$overall[2], gbm.confusion.matrix$overall[2], knn.confusion.matrix$overall[2],lvq.confusion.matrix$overall[2], nb.confusion.matrix$overall[2], nnet.confusion.matrix$overall[2], rf.confusion.matrix$overall[2], rpart.confusion.matrix$overall[2], svm.confusion.matrix$overall[2])

user.self <- c((baseline.time[1]*baseline.scaler)+baseline.program.time, gbm.time[1], knn.time[1], lvq.time[1], nb.time[1], nnet.time[1], rf.time[1], rpart.time[1], svm.time[1])

sys.self <- c((baseline.time[2]*baseline.scaler), gbm.time[2], knn.time[2], lvq.time[2], nb.time[2], nnet.time[2], rf.time[2], rpart.time[2], svm.time[2])

elapsed <- c((baseline.time[3]*baseline.scaler)+baseline.program.time, gbm.time[3], knn.time[3], lvq.time[3], nb.time[3], nnet.time[3], rf.time[3], rpart.time[3], svm.time[3])

compare <- data.frame(model.name, accuracy, kappa, user.self, sys.self, elapsed)
```

Five of the eight machine learning models out prefomed the baseline.  The Naive Bayes, Learning Vector Quantization and K Nearest Neighbors did not preform as well as the crude baseline rule.

```{r, echo=FALSE}
ggplot(data=compare, aes(x=model.name, y=accuracy*100, fill=model.name)) + geom_bar(stat='identity') + coord_flip() + ggtitle('Figure 1: Model Accurately Predicted Survivorship') + theme(axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position='none') + scale_y_continuous(limits=c(0,100)) + scale_fill_brewer(palette='Set1')
```

When looking at the more concervative kappa measure, the same five of the machine learning algorythms outpreformed the baseline model.

```{r, echo=FALSE}
ggplot(data=compare, aes(x=model.name, y=kappa*100, fill=model.name)) + geom_bar(stat='identity') + coord_flip() + ggtitle("Figure 2: Cohen's Kappa") + theme(axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position='none') +  scale_y_continuous(limits=c(0,100)) + ylab('Percent') + scale_fill_brewer(palette='Set1')
```

The is considerable difference in the amount of time needed to generate the models.  Remember we assumed that it took one minute to program the baseline model.


```{r, echo=FALSE}
ggplot(data=compare, aes(x=model.name, y=elapsed, fill=model.name)) + geom_bar(stat='identity') + coord_flip() + ggtitle('Figure 3: Execution Time (in seconds)') + theme(axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position='none') +  scale_y_continuous(limits=c(0,100)) + scale_fill_brewer(palette='Set1')
```

Now that we have a data set we can compare on I will be creating some index numbers.  The first will be an accuracy index which is an equal weighting of both accuracy measures.  The second will be a speed index which is based on the execution time.  Each index is computed relative to the baseline and scaled so the baseline equals 100.

```{r}
compare <- compare %>%
  mutate(accuracy.index = ((accuracy + kappa)/2)) %>%
  mutate(speed.index = elapsed)

baseline.accuracy <- compare[compare$model.name=='Baseline',]$accuracy.index

baseline.speed <- compare[compare$model.name=='Baseline',]$speed.index

compare <- compare %>%
  mutate(accuracy.index = (accuracy.index/baseline.accuracy)*100) %>%
  mutate(speed.index = (compare$speed.index/baseline.speed)*100)
```

Now we can examine the model's preformance in terms of speed and accuracy:

```{r, echo=FALSE}
ggplot(data=compare, aes(x=speed.index, y=accuracy.index, color=model.name)) + geom_point(stat='identity') + ggtitle('Figure 4: Accuracy vs Execution Time') + xlab('Speed Index (Baseline=100)') + ylab('Accuracy Index (Baseline=100)') + scale_color_brewer(palette='Set1') + guides(color=guide_legend(title='Model Name'))
```

I you only cared about the accuracy this study suggest the Gradient Boosted Machine would be a good choice.  However if you only cared about speed the K Nearest Neighbors approach would be the best one to use.  In order to evaluate which model would be the best along the accuracy vs speed tradeoff space I will weigh the index values

```{r}

recommendation <- function(accuracy.weight){
  speed.weight <- 1- accuracy.weight
  accuracy.component <- compare$accuracy.index * accuracy.weight
  speed.component <- compare$speed.index * speed.weight
  score <- accuracy.component - speed.component
  compare[order(-score),]$model.name
}

for(i in seq(0, 1, 0.1)){
  row <- data.frame(model=recommendation(i), accuracy=i*100, rank=1:9)
  if(!exists('recommendations'))
    recommendations <- row
  else
    recommendations <- rbind(recommendations, row)
}

# library(tidyr)
# x <- recommendations %>% spread(accuracy, rank)
# row.names(x) <- x[,1]
# x <- data.matrix(x[,-1])
# heatmap(x, Rowv=NA, Colv=NA)
```
# Appendix

## Titanic Survivor Dataset

```{r, echo=FALSE, message=FALSE}
if (!require("DT")) devtools::install_github("rstudio/DT")
library(DT)
datatable(titanic, options = list(iDisplayLength = 10))
```

## Machine Learning Models

```{r}
gbm.model
knn.model
lvq.model
nb.model
nnet.model
rf.model
rpart.model
svm.model
```

### Resampling Results

```{r}
results <- resamples(
  list(
    GBM=gbm.model, 
    KNN=knn.model, 
    LVQ=lvq.model, 
    NB=nb.model, 
    NNET=nnet.model, 
    RF=rf.model, 
    RPART=rpart.model, 
    SVM=svm.model
  )
)

summary(results)
```


```{r, echo=FALSE}
bwplot(results)
```

## Comparison Values

```{r, echo=FALSE}
datatable(compare, options = list(iDisplayLength = 9, searching=FALSE, paging=FALSE))
```

## Session Info
```{r}
sessionInfo()
```